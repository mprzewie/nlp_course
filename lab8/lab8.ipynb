{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab8 - Text classification\n",
    "\n",
    "The task concentrates on content-based text the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-57cd770d2b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mskm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfasttext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msupervised\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaggedCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_fetcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLPTaskDataFetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNLPTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlairEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDocumentRNNEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/ml/lib/python3.7/site-packages/flair/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/ml/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m __all__ += [name for name in dir(_C)\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             not name.endswith('Base')]\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import dataclasses as dc\n",
    "from typing import List\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics as skm\n",
    "from fasttext import supervised as fasttext\n",
    "from flair.data import TaggedCorpus, Sentence\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dc.dataclass\n",
    "class Dataset:\n",
    "    X_train: List\n",
    "    y_train: List\n",
    "    X_val: List\n",
    "    y_val: List    \n",
    "    X_test: List\n",
    "    y_test: List\n",
    "        \n",
    "    def preprocess(self, fn):\n",
    "        return dc.replace(\n",
    "            self,\n",
    "            X_train = [fn(x) for x in self.X_train],\n",
    "            X_val = [fn(x) for x in self.X_val],\n",
    "            X_test = [fn(x) for x in self.X_test],\n",
    "         )\n",
    "    \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self.X_train + self.X_val + self.X_test\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.y_train + self.y_val + self.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Divide the set of bills into two exclusive sets:\n",
    "   1. the set of bills amending other bills (their title starts with `o zmianie ustawy`),\n",
    "   1. the set of bills not amending other bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = Path(\"../data\").glob(\"*.txt\")\n",
    "bills = [f.open().read().replace(\"  \", \" \").lower() for f in data_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_changing_bill(text):\n",
    "    return \"o zmianie ustawy\" in text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(692, 488)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives_raw = [b for b in bills if is_changing_bill(b)]\n",
    "negatives_raw = [b for b in bills if not is_changing_bill(b)]\n",
    "\n",
    "len(positives_raw), len(negatives_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Change the contents of the bill by removing the date of publication and the title (so the words `o zmianie ustawy`\n",
    "   are removed).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives, negatives = [[\"\".join(b.split(\"art. 1\")[1:]) for b in ds] for ds in [positives_raw, negatives_raw]]\n",
    "X_ = positives + negatives\n",
    "y_ = ([1] * len(positives)) + ([0] * len(negatives))\n",
    "\n",
    "X = [x for x in X_ if len(x) > 0]\n",
    "y = [y for (x, y) in zip(X_, y_) if len(x) > 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the sets of documents into the following groups by randomly selecting the documents:\n",
    "   1. 60% training\n",
    "   1. 20% validation\n",
    "   1. 20% testing\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5)\n",
    "\n",
    "original_ds = Dataset(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    X_test,\n",
    "    y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Do not change these groups during the following experiments.\n",
    "5. Prepare the following variants of the documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   a. full text of the document\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   b. randomly selected 10% of the lines of the document\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenpercentlines(text):\n",
    "    lines = [l for l in text.split(\"\\n\") if l != \"\"]\n",
    "    return \"\\n\".join(np.random.choice(lines, (len(lines) // 10) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   c. randomly selected 10 lines of the document\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenlines(text):\n",
    "    lines = [l for l in text.split(\"\\n\") if l != \"\"]\n",
    "    return \" \".join(np.random.choice(lines, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   d. randomly selected 1 line of the document\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneline(text):\n",
    "    lines = [l for l in text.split(\"\\n\") if l != \"\"]\n",
    "    return \" \".join(np.random.choice(lines, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Train the following classifiers on the documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   a. SVM with TF•IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_svm(ds: Dataset):\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer().fit(ds.X)),\n",
    "        ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        \"clf__estimator__C\": [0.01, 0.1, 0.5, 1],\n",
    "        \"clf__estimator__class_weight\": ['balanced', None],\n",
    "    }\n",
    "    grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=12, verbose=10)\n",
    "    grid_search_tune.fit(ds.X_train, ds.y_train)\n",
    "    \n",
    "    return grid_search_tune.best_estimator_.predict(ds.X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_ds(ds: Dataset):\n",
    "    out = Path(\"/tmp\") / str(np.random.randn())\n",
    "    out.mkdir()\n",
    "    for d in [\"train\", \"val\", \"test\"]:\n",
    "        X_d, y_d = getattr(ds, f\"X_{d}\"), getattr(ds, f\"y_{d}\")\n",
    "        txt = \"\\n\".join([\n",
    "          f\"__label__{y} {x}\"  \n",
    "            for (x,y) in zip(X_d, y_d)\n",
    "        ])\n",
    "        with (out / d).open(\"w\") as f:\n",
    "            f.write(txt)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_supervised(ds: Dataset):\n",
    "    ds_path = fasttext_ds(original_ds.preprocess(oneline))\n",
    "    ft = fasttext(\n",
    "        str(ds_path / \"train\"), str(ds_path / \"model\"),\n",
    "        epoch=10, \n",
    "#         word_ngrams=2, #kills the kernel\n",
    "    )\n",
    "    return [int(y[0]) for y in ft.predict(ds.X_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Flair with Polish language model\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_model(ds: Dataset):\n",
    "\n",
    "    data_folder = fasttext_ds(ds)\n",
    "    corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(\n",
    "        str(data_folder),\n",
    "        test_file='test',\n",
    "        dev_file='val',\n",
    "        train_file='train'\n",
    "    )\n",
    "\n",
    "\n",
    "    # 3. make a list of word embeddings\n",
    "    word_embeddings = [\n",
    "        WordEmbeddings('pl'), \n",
    "        FlairEmbeddings('polish-forward'),\n",
    "       FlairEmbeddings('polish-backward')\n",
    "    ]\n",
    "        \n",
    "\n",
    "    # 4. initialize document embedding by passing list of word embeddings\n",
    "    # Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "    document_embeddings: DocumentLSTMEmbeddings = DocumentLSTMEmbeddings(\n",
    "        word_embeddings,\n",
    "        hidden_size=512,\n",
    "        reproject_words=True,\n",
    "        reproject_words_dimension=256,\n",
    "        )\n",
    "\n",
    "    # 5. create the text classifier\n",
    "    classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "\n",
    "    # 6. initialize the text classifier trainer\n",
    "    trainer = ModelTrainer(\n",
    "        classifier, \n",
    "        corpus, \n",
    "#         optimizer=torch.optim.Adam()\n",
    "    )\n",
    "\n",
    "    # 7. start the training\n",
    "    model_path =  str(data_folder / \"model\"/ \"best-model.pt\")\n",
    "    trainer.train( \n",
    "        model_path,\n",
    "        learning_rate=0.1,\n",
    "        mini_batch_size=32,\n",
    "        anneal_factor=0.5,\n",
    "        patience=5,\n",
    "        max_epochs=1,\n",
    "        monitor_train=False\n",
    "    )    \n",
    "    return [\n",
    "        y.labels[0]\n",
    "        for y in \n",
    "        trainer.model.predict([Sentence(x) for x in ds.X_val])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Report Precision, Recall and F1 for each variant of the experiment (12 variants altogether)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_gt, y_pred):\n",
    "    return {\n",
    "        fn.__name__: fn(y_gt, y_pred)\n",
    "        for fn in [\n",
    "            skm.accuracy_score,\n",
    "            skm.precision_score,\n",
    "            skm.recall_score,\n",
    "            skm.f1_score\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=12)]: Done  37 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=12)]: Done  48 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=12)]: Done  61 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=12)]: Done  74 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=12)]: Done  89 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=12)]: Done 104 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=12)]: Done 121 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=12)]: Done 136 out of 144 | elapsed:    4.4s remaining:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 144 out of 144 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=12)]: Done  37 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=12)]: Done  48 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=12)]: Done  61 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=12)]: Done  74 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=12)]: Done  89 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=12)]: Done 104 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=12)]: Done 121 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=12)]: Done 136 out of 144 | elapsed:   10.9s remaining:    0.6s\n",
      "[Parallel(n_jobs=12)]: Done 144 out of 144 | elapsed:   11.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=12)]: Done  37 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=12)]: Done  48 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=12)]: Done  61 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=12)]: Done  74 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=12)]: Done  89 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=12)]: Done 104 tasks      | elapsed:   38.8s\n",
      "[Parallel(n_jobs=12)]: Done 121 tasks      | elapsed:   45.8s\n",
      "[Parallel(n_jobs=12)]: Done 136 out of 144 | elapsed:   51.1s remaining:    3.0s\n",
      "[Parallel(n_jobs=12)]: Done 144 out of 144 | elapsed:   53.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:   38.9s\n",
      "/home/mprzewie/.anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=12)]: Done  37 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=12)]: Done  48 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=12)]: Done  61 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=12)]: Done  74 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=12)]: Done  89 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=12)]: Done 104 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=12)]: Done 121 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=12)]: Done 136 out of 144 | elapsed:  7.9min remaining:   27.8s\n",
      "[Parallel(n_jobs=12)]: Done 144 out of 144 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-932ebc80b4ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     }.items()\n\u001b[1;32m     20\u001b[0m }\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    ds_name: {\n",
    "        model_fn.__name__: metrics(ds.y_val, model_fn(ds))\n",
    "        for model_fn in [\n",
    "            tfidf_svm,\n",
    "            fasttext_supervised,\n",
    "#             flair_model\n",
    "        ]\n",
    "    }\n",
    "    for (ds_name, ds) in {\n",
    "        fn.__name__: original_ds.preprocess(fn) \n",
    "        for fn in\n",
    "        [\n",
    "            oneline,\n",
    "            tenlines, \n",
    "            tenpercentlines, \n",
    "            full_text, \n",
    "        ]\n",
    "    }.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oneline': {'tfidf_svm': {'accuracy_score': 0.591304347826087,\n",
       "   'precision_score': 0.5714285714285714,\n",
       "   'recall_score': 0.943089430894309,\n",
       "   'f1_score': 0.7116564417177914},\n",
       "  'fasttext_supervised': {'accuracy_score': 0.5826086956521739,\n",
       "   'precision_score': 0.5678391959798995,\n",
       "   'recall_score': 0.9186991869918699,\n",
       "   'f1_score': 0.7018633540372671}},\n",
       " 'tenlines': {'tfidf_svm': {'accuracy_score': 0.8,\n",
       "   'precision_score': 0.7851851851851852,\n",
       "   'recall_score': 0.8617886178861789,\n",
       "   'f1_score': 0.8217054263565892},\n",
       "  'fasttext_supervised': {'accuracy_score': 0.5478260869565217,\n",
       "   'precision_score': 0.5418502202643172,\n",
       "   'recall_score': 1.0,\n",
       "   'f1_score': 0.7028571428571428}},\n",
       " 'tenpercentlines': {'tfidf_svm': {'accuracy_score': 0.782608695652174,\n",
       "   'precision_score': 0.7588652482269503,\n",
       "   'recall_score': 0.8699186991869918,\n",
       "   'f1_score': 0.8106060606060606},\n",
       "  'fasttext_supervised': {'accuracy_score': 0.6,\n",
       "   'precision_score': 0.5720930232558139,\n",
       "   'recall_score': 1.0,\n",
       "   'f1_score': 0.7278106508875739}},\n",
       " 'full_text': {'tfidf_svm': {'accuracy_score': 0.8695652173913043,\n",
       "   'precision_score': 0.8394160583941606,\n",
       "   'recall_score': 0.9349593495934959,\n",
       "   'f1_score': 0.8846153846153846},\n",
       "  'fasttext_supervised': {'accuracy_score': 0.5695652173913044,\n",
       "   'precision_score': 0.5540540540540541,\n",
       "   'recall_score': 1.0,\n",
       "   'f1_score': 0.7130434782608697}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "\n",
    "1. Application of SVM classifier with TF•IDF is described in \n",
    "   [David Batista](http://www.davidsbatista.net/blog/2017/04/01/document_classification/) blog post.\n",
    "1. [Fasttext](https://fasttext.cc/) is a popular basline classifier. Don't report the Precision/Recall/F1 provided by\n",
    "   Fasttext since they might be [wrong](https://github.com/facebookresearch/fastText/issues/261).\n",
    "1. [Flair](https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f) \n",
    "   is another library for text processing. Flair classification is based on a language model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
