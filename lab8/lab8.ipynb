{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab8 - Text classification\n",
    "\n",
    "The task concentrates on content-based text the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import dataclasses as dc\n",
    "from typing import List\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics as skm\n",
    "from fasttext import supervised as fasttext\n",
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dc.dataclass\n",
    "class Dataset:\n",
    "    X_train: List\n",
    "    y_train: List\n",
    "    X_val: List\n",
    "    y_val: List    \n",
    "    X_test: List\n",
    "    y_test: List\n",
    "        \n",
    "    def preprocess(self, fn):\n",
    "        return dc.replace(\n",
    "            self,\n",
    "            X_train = [fn(x) for x in self.X_train],\n",
    "            X_val = [fn(x) for x in self.X_val],\n",
    "            X_test = [fn(x) for x in self.X_test],\n",
    "         )\n",
    "    \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self.X_train + self.X_val + self.X_test\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.y_train + self.y_val + self.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Divide the set of bills into two exclusive sets:\n",
    "   1. the set of bills amending other bills (their title starts with `o zmianie ustawy`),\n",
    "   1. the set of bills not amending other bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = Path(\"../data\").glob(\"*.txt\")\n",
    "bills = [f.open().read().replace(\"  \", \" \").lower() for f in data_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_changing_bill(text):\n",
    "    return \"o zmianie ustawy\" in text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(692, 488)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives_raw = [b for b in bills if is_changing_bill(b)]\n",
    "negatives_raw = [b for b in bills if not is_changing_bill(b)]\n",
    "\n",
    "len(positives_raw), len(negatives_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Change the contents of the bill by removing the date of publication and the title (so the words `o zmianie ustawy`\n",
    "   are removed).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives, negatives = [[\"\".join(b.split(\"art. 1\")[1:]) for b in ds] for ds in [positives_raw, negatives_raw]]\n",
    "X_ = positives + negatives\n",
    "y_ = ([1] * len(positives)) + ([0] * len(negatives))\n",
    "\n",
    "X = [x for x in X_ if len(x) > 0]\n",
    "y = [y for (x, y) in zip(X_, y_) if len(x) > 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the sets of documents into the following groups by randomly selecting the documents:\n",
    "   1. 60% training\n",
    "   1. 20% validation\n",
    "   1. 20% testing\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5)\n",
    "\n",
    "original_ds = Dataset(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    X_test,\n",
    "    y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Do not change these groups during the following experiments.\n",
    "5. Prepare the following variants of the documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   a. full text of the document\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   b. randomly selected 10% of the lines of the document\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenpercentlines(text):\n",
    "    lines = [l for l in text.split(\"\\n\") if l != \"\"]\n",
    "    return \"\\n\".join(np.random.choice(lines, (len(lines) // 10) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   c. randomly selected 10 lines of the document\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenlines(text):\n",
    "    lines = [l for l in text.split(\"\\n\") if l != \"\"]\n",
    "    return \" \".join(np.random.choice(lines, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   d. randomly selected 1 line of the document\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneline(text):\n",
    "    lines = [l for l in text.split(\"\\n\") if l != \"\"]\n",
    "    return \" \".join(np.random.choice(lines, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Train the following classifiers on the documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   a. SVM with TFâ€¢IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_svm(ds: Dataset):\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer().fit(ds.X)),\n",
    "        ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        \"clf__estimator__C\": [0.01, 0.1, 0.5, 1],\n",
    "        \"clf__estimator__class_weight\": ['balanced', None],\n",
    "    }\n",
    "    grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=12, verbose=10)\n",
    "    grid_search_tune.fit(ds.X_train, ds.y_train)\n",
    "    \n",
    "    return grid_search_tune.best_estimator_.predict(ds.X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_ds(ds: Dataset):\n",
    "    out = Path(\"/tmp\") / str(np.random.randn())\n",
    "    out.mkdir()\n",
    "    for d in [\"train\", \"val\", \"test\"]:\n",
    "        X_d, y_d = getattr(ds, f\"X_{d}\"), getattr(ds, f\"y_{d}\")\n",
    "        txt = \"\\n\".join([\n",
    "          f\"__label__{y} {x}\"  \n",
    "            for (x,y) in zip(X_d, y_d)\n",
    "        ])\n",
    "        with (out / d).open(\"w\") as f:\n",
    "            f.write(txt)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_supervised(ds: Dataset):\n",
    "    ds_path = fasttext_ds(original_ds.preprocess(oneline))\n",
    "    ft = fasttext(\n",
    "        str(ds_path / \"train\"), str(ds_path / \"model\"),\n",
    "        epoch=10, \n",
    "#         word_ngrams=2, #kills the kernel\n",
    "    )\n",
    "    return [int(y[0]) for y in ft.predict(ds.X_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Flair with Polish language model\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_model(ds: Dataset):\n",
    "\n",
    "    data_folder = fasttext_ds(ds)\n",
    "    corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(\n",
    "        str(data_folder),\n",
    "        test_file='test',\n",
    "        dev_file='val',\n",
    "        train_file='train'\n",
    "    )\n",
    "    # 1. get the corpus\n",
    "\n",
    "    # 2. create the label dictionary\n",
    "    label_dict = corpus.make_label_dictionary()\n",
    "\n",
    "    # 3. make a list of word embeddings\n",
    "    word_embeddings = [\n",
    "        WordEmbeddings('pl'), \n",
    "         FlairEmbeddings('polish-forward'),\n",
    "                       FlairEmbeddings('polish-backward')]\n",
    "        \n",
    "\n",
    "    # 4. initialize document embedding by passing list of word embeddings\n",
    "    # Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "    document_embeddings: DocumentLSTMEmbeddings = DocumentRNNEmbeddings(\n",
    "        word_embeddings,\n",
    "        hidden_size=512,\n",
    "        reproject_words=True,\n",
    "        reproject_words_dimension=256,\n",
    "        )\n",
    "\n",
    "    # 5. create the text classifier\n",
    "    classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False)\n",
    "\n",
    "    # 6. initialize the text classifier trainer\n",
    "    trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "    # 7. start the training\n",
    "    model_path =  str(data_folder / \"model\")\n",
    "    trainer.train( \n",
    "        model_path,\n",
    "        learning_rate=0.1,\n",
    "        mini_batch_size=32,\n",
    "        anneal_factor=0.5,\n",
    "        patience=5,\n",
    "        max_epochs=10\n",
    "    )\n",
    "    model = TextClassifier.load_from_file(model_path)\n",
    "    \n",
    "    return [\n",
    "        model.predict(Sentence(x))\n",
    "        for x in ds.X_val\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Report Precision, Recall and F1 for each variant of the experiment (12 variants altogether)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_gt, y_pred):\n",
    "    return {\n",
    "        fn.__name__: fn(y_gt, y_pred)\n",
    "        for fn in [\n",
    "            skm.accuracy_score,\n",
    "            skm.precision_score,\n",
    "            skm.recall_score,\n",
    "            skm.f1_score\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=12)]: Done  37 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=12)]: Done  48 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=12)]: Done  61 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=12)]: Done  74 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=12)]: Done  89 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=12)]: Done 104 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=12)]: Done 121 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=12)]: Done 136 out of 144 | elapsed:   13.8s remaining:    0.8s\n",
      "[Parallel(n_jobs=12)]: Done 144 out of 144 | elapsed:   14.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Batch computation too fast (0.0898s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done  32 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=12)]: Done  76 out of 144 | elapsed:    1.5s remaining:    1.4s\n",
      "[Parallel(n_jobs=12)]: Done 136 out of 144 | elapsed:    2.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 144 out of 144 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oneline': {'fasttext_supervised': {'accuracy_score': 0.6043478260869565,\n",
      "                                     'f1_score': 0.7331378299120234,\n",
      "                                     'precision_score': 0.6067961165048543,\n",
      "                                     'recall_score': 0.9259259259259259},\n",
      "             'tfidf_svm': {'accuracy_score': 0.6043478260869565,\n",
      "                           'f1_score': 0.7377521613832854,\n",
      "                           'precision_score': 0.6037735849056604,\n",
      "                           'recall_score': 0.9481481481481482}},\n",
      " 'tenlines': {'fasttext_supervised': {'accuracy_score': 0.6173913043478261,\n",
      "                                      'f1_score': 0.7541899441340781,\n",
      "                                      'precision_score': 0.6053811659192825,\n",
      "                                      'recall_score': 1.0},\n",
      "              'tfidf_svm': {'accuracy_score': 0.7652173913043478,\n",
      "                            'f1_score': 0.7985074626865671,\n",
      "                            'precision_score': 0.8045112781954887,\n",
      "                            'recall_score': 0.7925925925925926}}}\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    ds_name: {\n",
    "        model_fn.__name__: metrics(ds.y_val, model_fn(ds))\n",
    "        for model_fn in [\n",
    "            tfidf_svm,\n",
    "            fasttext_supervised,\n",
    "            flair_model\n",
    "        ]\n",
    "    }\n",
    "    for (ds_name, ds) in {\n",
    "        fn.__name__: original_ds.preprocess(fn) \n",
    "        for fn in\n",
    "        [\n",
    "            full_text, \n",
    "            tenpercentlines, \n",
    "            tenlines, \n",
    "            oneline\n",
    "        ]\n",
    "    }.items()\n",
    "}\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "\n",
    "1. Application of SVM classifier with TFâ€¢IDF is described in \n",
    "   [David Batista](http://www.davidsbatista.net/blog/2017/04/01/document_classification/) blog post.\n",
    "1. [Fasttext](https://fasttext.cc/) is a popular basline classifier. Don't report the Precision/Recall/F1 provided by\n",
    "   Fasttext since they might be [wrong](https://github.com/facebookresearch/fastText/issues/261).\n",
    "1. [Flair](https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f) \n",
    "   is another library for text processing. Flair classification is based on a language model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
