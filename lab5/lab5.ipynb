{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 5 - Morphosyntactic tagging\n",
    "\n",
    "Morphosyntactic tagging is one of the core algorithms in NLP. It assigns morphological\n",
    "and (in some languages) syntactic tags to the words in a text. E.g. this allows to distinguish\n",
    "between the major grammatical categories, such as nouns and verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download [docker image](https://hub.docker.com/r/djstrong/krnnt2) o KRNNT2. It includes the following tools:\n",
    "   1. Morfeusz2 - morphological dictionary\n",
    "   1. Corpus2 - corpus access library\n",
    "   1. Toki - tokenizer for Polish\n",
    "   1. Maca - morphosyntactic analyzer\n",
    "   1. rknnt - Polish tagger\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the tool to tag and lemmatize the corpus with the bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def krnnt(text, url=\"http://localhost:9200\"):\n",
    "    result = req.post(url, text.encode(\"utf-8\")).content.decode(\"utf-8\").split(\"\\n\")\n",
    "    grouped = [result[2*i:2*i + 2] for i in range(int(len(result) /2))][:-1]\n",
    "    result = [g[1].split(\"\\t\")[1:3] for g in grouped]\n",
    "    result = [r for r in result if len(r) == 2]\n",
    "    result = [(r[0], r[1].split(\":\")[0]) for r in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krnnt(\"Bedę lematyzował\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the tagged corpus compute bigram statistic for the tokens containing:\n",
    "   1. lemmatized, downcased word\n",
    "   1. morphosyntactic category of the word (noun, verb, etc.)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = []\n",
    "for file in tqdm(list(Path(\"../data/\").glob(\"*.txt\"))):\n",
    "    with file.open() as f:\n",
    "        text = f.read()\n",
    "    lemmatized = krnnt(text)\n",
    "    corpora.append(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Exclude bigram containing non-words (such as numbers, interpunction, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_corpora = [\n",
    "    [\n",
    "        c for c in corp if c[0].isalpha()\n",
    "    ]\n",
    "    for corp in corpora\n",
    "]\n",
    "word_corpora[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = defaultdict(int)\n",
    "for corp in word_corpora:\n",
    "    for w in corp:\n",
    "        words[w] = words[w] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ngram = Tuple[Tuple[str, str], Tuple[str, str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = defaultdict(int)\n",
    "\n",
    "for corp in word_corpora:\n",
    "    corp_bigrams = zip(corp[:-1], corp[1:])\n",
    "    for b in corp_bigrams:\n",
    "        bigrams[b] = bigrams[b] + 1\n",
    "\n",
    "list(bigrams.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute LLR statistic for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(k: np.ndarray):\n",
    "    N = k.sum()\n",
    "    return (\n",
    "        (k / N) * np.log(k / N + 1e-7)\n",
    "    ).sum()\n",
    "\n",
    "def LLR(k: np.ndarray):\n",
    "    return (2 * k.sum()) * (\n",
    "        H(k) - \n",
    "        H(k.sum(axis=0)) - \n",
    "        H(k.sum(axis=1))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ngrams = np.sum(list(bigrams.values()))\n",
    "\n",
    "def incidence(bigram: Ngram, ngrams: Dict[Ngram, int], words: Dict[Tuple[str, str], int]) -> np.ndarray:\n",
    "    w1, w2 = bigram\n",
    "    w1_w2 = ngrams.get((w1, w2), 0)\n",
    "    w1_not_w2 = words[w1] - w1_w2\n",
    "    w2_not_w1 = words[w2] - w1_w2\n",
    "    not_w1_not_w2 = all_ngrams - w1_not_w2 - w1_not_w2 - w1_w2\n",
    "    return np.array([\n",
    "        [w1_w2, w1_not_w2],\n",
    "        [w2_not_w1, not_w1_not_w2]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgrams_llrs = {\n",
    "    ngram: LLR(incidence(ngram, bigrams, words))\n",
    "    for ngram in bigrams\n",
    "}\n",
    "bgrams_llrs;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Select top 50 results including noun at the first position and noun or adjective at the second position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    k: bgrams_llrs[k]\n",
    "    for k in sorted(\n",
    "        [\n",
    "            ((w1, a1), (w2, a2)) \n",
    "            for ((w1, a1), (w2, a2)) in bgrams_llrs\n",
    "            if a1 == \"subst\" and a2 == \"adj\"\n",
    "        ],\n",
    "        key = lambda k: -bgrams_llrs[k]\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "1. A morphosyntactic analyzer provides the possible values of morphosyntactic tags for the words.\n",
    "   E.g. for Polish \"ma\" word it can produce the following interpretations:\n",
    "   ``` \n",
    "    ma\tspace\n",
    "            mieć\tfin:sg:ter:imperf\n",
    "            mój  \tadj:sg:nom:f:pos\n",
    "            mój  \tadj:sg:voc:f:pos\n",
    "   ```\n",
    "   1. The first interpretation shows that the word can be a verb in singular, in 3rd person.\n",
    "   1. The second interpretation shows that the word can be an adjective in singular, in nominative, in feminine.\n",
    "   1. The third interpretation shows that the word can be an adjective in singular, in vocative, in feminine.\n",
    "1. The full list of tags is available at [NKJP](http://nkjp.pl/poliqarp/help/ense2.html).\n",
    "1. A morphosyntactic tagger selects one of the interpretation of a word, taking into account its context.\n",
    "   It can take the interpretation from a dictionary (like KRNNT), but it can also compute it dynamically (e.g. \n",
    "   [COMBO](https://github.com/360er0/COMBO) is a tagger that does not need a morphosyntactic ananlyzer).\n",
    "1. The information provided by a tagger can be useful for many applications. You can selects words from particular\n",
    "   grammatical category or you can submit the data to a downstream task such as text classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
